# -*- coding: utf-8 -*-
"""Trigram Project With Class Function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aua8LmkbixiWKtqV7xvvbcqYjUTrXCnD

# New way to implement Trigram
"""

import nltk
nltk.download('brown')
from collections import Counter
import string
import re
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from nltk.corpus import brown
from random import choices
from random import randint

X=[]
file=brown.sents()
n=len(brown.sents())

for i in range(n):
  s = re.sub(r'\W+',' ',str(file[i])).strip()
  s = re.sub(r'\d+','',s).strip()
  s = re.split(r'\s+',s.lower())
  X.append(s)
X_train, X_test = train_test_split(X,test_size=0.2, random_state=1)

class data_prep:
  def __init__(self,train_data,test_data,n_gram=int(),threshold=int()):
    self.train=train_data
    self.test=test_data
    self.ng=n_gram
    self.t=threshold

  def n_list(self,data):
    data_list=[]
    for i in data:
      for j in range(self.ng-1):
        data_list.append('<s>')
      for k in i:
        data_list.append(k)
      data_list.append('</s>')
    return(data_list)
  
  def cut_off(self,data_list):
    data_vocab=Counter(data_list)
    count_sum=0
    data_vocab_new={}
    for i in data_vocab:
      if data_vocab[i]<self.t:
        count_sum+=data_vocab[i]
      else:
        data_vocab_new[i]=data_vocab[i]
    data_vocab_new['<UNK>']=count_sum
    return(data_vocab_new)

  def replace_unk(self,data_list,vocab):
    data_list_replace=[]
    for i in data_list:
      if i in vocab:
        data_list_replace.append(i)
      else:
        data_list_replace.append('<UNK>')
    return(data_list_replace)

  def transform(self):
    self.train_list=self.n_list(self.train)
    self.test_list=self.n_list(self.test)
    self.data_vocab=self.cut_off(self.train_list)
    self.train_list_new=self.replace_unk(self.train_list,self.data_vocab)
    self.test_list_new=self.replace_unk(self.test_list,self.data_vocab)
    return(self.train_list_new,self.test_list_new,self.data_vocab)

class prob_transform:
  def __init__(self,data,n_gram=int()):
    self.data=data
    self.ng=n_gram

  def transform(self):
    l=len(self.data)
    if self.ng == 1:
      return(Counter(self.data))
    else:
      data_prob={}
      for i in range(l-self.ng+1):
        temp_key=tuple(self.data[i:i+self.ng-1])
        if temp_key not in data_prob:
          data_prob[temp_key]={}
          data_prob[temp_key][self.data[i+self.ng-1]]=1
        else:
          if self.data[i+self.ng-1] not in data_prob[temp_key]:
            data_prob[temp_key][self.data[i+self.ng-1]]=1
          else:
            data_prob[temp_key][self.data[i+self.ng-1]]+=1
      return(data_prob)

class smoothing:
  def __init__(self,test_data,prob_dict,vocab,n_gram=int()):
    self.data=test_data
    self.prob=prob_dict
    self.vocab=vocab
    self.ng=n_gram

  def calc(self,lamb=double()):
    n=len(self.data)
    m=len(self.vocab)
    entro=0
    if self.ng ==1:
      sum=0
      for j in self.prob:
        sum+=self.prob[j]
      for i in self.data:
        c=self.prob[i]
        prob=(c+lamb)/(sum+m*lamb)
        entro+= -np.log2(prob)
    else:
      for i in range(n-self.ng+1):
        if self.data[i+self.ng-2]=='</s>':
          continue
        else:
          sum = 0
          end=i+self.ng-1
          temp_key=tuple(self.data[i:end])
          if temp_key not in self.prob:
            entro+=np.log2(m)
          else:
            for k in self.prob[temp_key]:
              sum+=self.prob[temp_key][k]
            if self.data[end] not in self.prob[temp_key]:
              c=0
            else:
              c=self.prob[temp_key][self.data[end]]
            prob=(c+lamb)/(sum+m*lamb) 
            entro+=-np.log2(prob)       
    entro=entro/n
    return(entro)

"""Unigram, Bigram and Trigram model training"""

threshold_para=[10,20,50,100]
m=KFold(n_splits=4)

lamb_para=[0.001,0.01,0.1,1,10,100,1000]
uni_result=[]
for t in threshold_para:
  uni_cex_sum=[0]*len(lamb_para)
  I=0
  for l in lamb_para:
    uni_cross_entro=0
    for X_training,X_vali in m.split(X_train):
      train=[X_train[i] for i in X_training]
      test=[X_train[j] for j in X_vali]
      uni_train_list,uni_test_list,uni_vocab=data_prep(train,test,1,t).transform()
      uni_prob=prob_transform(uni_train_list,1).transform()
      uni_cross_entro+=smoothing(uni_test_list,uni_prob,uni_vocab,1).calc(l)
    uni_cex_sum[I]=uni_cross_entro
    I=I+1
  uni_result.append(uni_cex_sum)

print(uni_result)

uni_best_lambda=lamb_para[uni_result[0].index(min(uni_result[0]))]

uni_train_list,uni_test_list,uni_vocab=data_prep(X_train,X_test,1,10).transform()
uni_prob=prob_transform(uni_train_list,1).transform()
print(2**smoothing(uni_test_list,uni_prob,uni_vocab,1).calc(uni_best_lambda))

for i in range(len(uni_result)):
  uni_entro=uni_result[i]
  uni_perplex=[2**(uni_entro[j]/4) for j in range(len(lamb_para))]
  plt.plot(lamb_para, uni_perplex,label='Threshold = %s' % threshold_para[i])
plt.xlabel('lambda')
plt.ylabel('Unigram perplexity')
plt.xscale('log')
plt.legend(loc=2)
plt.show()

lamb_para=[0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1,10]
tri_result=[]
bi_result=[]

for t in threshold_para:
  bi_cex_sum=[0]*len(lamb_para)
  I=0
  for l in lamb_para:
    bi_cross_entro=0
    for X_training,X_vali in m.split(X_train):
      train=[X_train[i] for i in X_training]
      test=[X_train[j] for j in X_vali]
      bi_train_list,bi_test_list,bi_vocab=data_prep(train,test,2,t).transform()
      bi_prob=prob_transform(bi_train_list,2).transform()
      del bi_vocab['<s>']
      bi_cross_entro+=smoothing(bi_test_list,bi_prob,bi_vocab,2).calc(l)
    bi_cex_sum[I]=bi_cross_entro
    I=I+1
  bi_result.append(bi_cex_sum)

print(bi_result)

for i in range(len(bi_result)):
  bi_entro=bi_result[i]
  bi_perplex=[2**(bi_entro[j]/4) for j in range(len(lamb_para))]
  plt.plot(lamb_para, bi_perplex,label='Threshold = %s' % threshold_para[i])
plt.xlabel('lambda')
plt.ylabel('Bigram perplexity')
plt.xscale('log')
plt.legend(loc=2)
plt.show()

bi_best_lambda=lamb_para[bi_result[0].index(min(bi_result[0]))]

bi_train_list,bi_test_list,bi_vocab=data_prep(X_train,X_test,2,10).transform()
bi_prob=prob_transform(bi_train_list,2).transform()
del bi_vocab['<s>']
print(2**smoothing(bi_test_list,bi_prob,bi_vocab,2).calc(bi_best_lambda))

for t in threshold_para:
  tri_cex_sum=[0]*len(lamb_para)
  I=0
  for l in lamb_para:
    tri_cross_entro=0
    for X_training,X_vali in m.split(X_train):
      train=[X_train[i] for i in X_training]
      test=[X_train[j] for j in X_vali]
      tri_train_list,tri_test_list,tri_vocab=data_prep(train,test,3,t).transform()
      tri_prob=trigram_prob_dict(tri_train_list)
      del tri_vocab['<s>']
      tri_cross_entro+=smoothing(tri_test_list,tri_prob,tri_vocab,l)
    tri_cex_sum[I]=tri_cross_entro
    I=I+1
  tri_result.append(tri_cex_sum)

print(tri_result)

for i in range(len(tri_result)):
  tri_entro=tri_result[i]
  tri_perplex=[2**(tri_entro[j]/4) for j in range(len(lamb_para))]
  plt.plot(lamb_para, tri_perplex,label='Threshold = %s' % threshold_para[i])
plt.xlabel('lambda')
plt.ylabel('Trigram perplexity')
plt.xscale('log')
plt.legend(loc=2)
plt.show()

tri_entro=tri_result[3]
tri_perplex=[2**(tri_entro[j]/4) for j in range(len(lamb_para))]
plt.plot(lamb_para, tri_perplex)
plt.xlabel('lambda')
plt.xlabel('lambda')
plt.ylabel('Trigram perplexity')
plt.xscale('log')
plt.show()

tri_best_lambda=0.005

tri_train_list,tri_test_list,tri_vocab=data_prep(X_train,X_test,3,10).transform()
del tri_vocab['<s>']
tri_prob=prob_transform(tri_train_list,3).transform()
print(2**smoothing(tri_test_list,tri_prob,tri_vocab,3).calc(tri_best_lambda))

"""Sentence Generation"""

tri_vocab_list=[]
for i in tri_vocab:
  tri_vocab_list.append(i)

import random

s='<s> <s>'
r0='<s>'
r1='<s>'
while True:
  temp_key=[]
  temp_value=[]
  if (r0,r1) not in tri_prob:
    r0=r1
    r1=random.choice(tri_vocab_list)
    if r1 != '</s>':
      s=s+' '+r1
    else:
      break
  else:
    for j in tri_vocab:
      if j not in tri_prob[(r0,r1)]:
        temp_key.append(j)
        temp_value.append(tri_best_lambda)
      else:
        temp_key.append(j)
        temp_value.append(tri_prob[(r0,r1)][j])
    r0=r1
    r1=choices(temp_key,temp_value)[0]
    if r1 != '</s>':
      s=s+' '+r1
    else:
      break
print(s+' </s>')

"""Back off"""